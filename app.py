import streamlit as st

# Fix for Streamlit Cloud (ChromaDB requires SQLite > 3.35)
try:
    __import__('pysqlite3')
    import sys
    sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')
except ImportError:
    pass

import chromadb
import google.generativeai as genai
import os
import time
import gc

# Import config with fallback
try:
    from config import GEMINI_API_KEY, GEMINI_MODEL, BGEEmbeddingFunction
except ImportError:
    GEMINI_API_KEY = os.getenv("GEMINI_API_KEY", "")
    GEMINI_MODEL = "gemini-pro"
    BGEEmbeddingFunction = None  # Handle missing import gracefully

from rag_engine import EnhancedRAGEngine  # Import new RAG engine
from map_service import get_map_service  # Import map service
from streamlit_folium import st_folium  # Import folium integration

# --- PAGE CONFIG ---
st.set_page_config(
    page_title="è‡ºå¤§æ ¡åœ’è¡Œæ”¿å°å¹«æ‰‹ã€NTU Admin Helperã€‘",
    page_icon="ğŸŒ´",
    layout="wide"
)

# --- INIT ---
DB_PATH = "./chroma_db"
COLLECTION_NAME = "ntu_assistant"

@st.cache_resource
def get_rag_engine():
    """Initialize Enhanced RAG Engine with Two-Stage Retrieval"""
    try:
        print("DEBUG: get_rag_engine() called. Initializing BGEEmbeddingFunction...")
        # Pass the specific embedding function to ensure dimension match (1024 for BGE-M3)
        ef = BGEEmbeddingFunction() if BGEEmbeddingFunction else None
        print(f"DEBUG: BGEEmbeddingFunction initialized: {ef is not None}")
        
        print(f"DEBUG: Initializing EnhancedRAGEngine with DB_PATH={DB_PATH}...")
        engine = EnhancedRAGEngine(
            db_path=DB_PATH, 
            collection_name=COLLECTION_NAME,
            embedding_function=ef
        )
        print("DEBUG: EnhancedRAGEngine initialized successfully.")
        return engine
    except Exception as e:
        print(f"ERROR: RAG Engine Initialization failed: {e}")
        st.error(f"RAG å¼•æ“åˆå§‹åŒ–å¤±æ•—: {e}")
        return None

@st.cache_resource
def get_map_service_cached():
    """Initialize and cache map service"""
    return get_map_service()

def retrieve_documents(engine, query, use_two_stage=True):
    """Retrieve documents using Enhanced RAG Engine
    
    Args:
        engine: EnhancedRAGEngine instance
        query: User query
        use_two_stage: Whether to use two-stage retrieval (default: True)
    
    Returns:
        Retrieved documents with metadata
    """
    results = engine.retrieve(query, use_two_stage=use_two_stage)
    return results

def rewrite_query_with_context(api_key, model_name, messages, current_query):
    """Rewrite the query using recent dialogue context."""
    if not api_key:
        return current_query
    genai.configure(api_key=api_key)
    model = genai.GenerativeModel(model_name)

    recent = messages[-6:]  # last 3 turns (user+assistant)
    dialogue = []
    for msg in recent:
        role = "ä½¿ç”¨è€…" if msg["role"] == "user" else "åŠ©ç†"
        dialogue.append(f"{role}: {msg['content']}")
    dialogue_text = "\n".join(dialogue).strip()

    prompt = f"""è«‹å°‡ä½¿ç”¨è€…çš„å•é¡Œæ”¹å¯«ç‚º**å®Œæ•´ã€å¯ç¨ç«‹æª¢ç´¢**çš„æŸ¥è©¢å¥ã€‚
è¦æ±‚ï¼š
1. å¿…é ˆçµåˆæœ€è¿‘å°è©±ä¸Šä¸‹æ–‡ï¼ˆåŒ…å«åŠ©ç†å›ç­”ï¼‰ï¼Œè£œå…¨ä»£è©èˆ‡å–®ä½åç¨±ã€‚
2. ä¸è¦åŠ å…¥å°è©±ä¸­ä¸å­˜åœ¨çš„æ–°è³‡è¨Šã€‚
3. åªè¼¸å‡ºæ”¹å¯«å¾Œçš„æŸ¥è©¢å¥ï¼Œå‹¿åŠ è¨»è§£ã€‚

ã€æœ€è¿‘å°è©±ã€‘
{dialogue_text}

ã€ä½¿ç”¨è€…æœ€æ–°å•é¡Œã€‘
{current_query}
"""
    try:
        response = model.generate_content(prompt)
        rewritten = response.text.strip()
        return rewritten if rewritten else current_query
    except Exception:
        return current_query

def generate_response(api_key, model_name, query, context_docs, user_identity=""):
    """Generate answer using Gemini Pro"""
    genai.configure(api_key=api_key)
    model = genai.GenerativeModel(model_name)
    
    # Build location hints from retrieved metadata
    location_lines = []
    seen_locations = set()
    for meta in context_docs.get('metadatas', [[]])[0]:
        if meta.get('type') == 'location':
            building = meta.get('building', '')
            floor = meta.get('floor', '')
            room = meta.get('room', '')
            unit = meta.get('unit_name') or meta.get('title', '')
            location_text = f"{building} {floor} {room}å®¤".strip()
            if not location_text:
                continue
            line = f"- {unit}ï¼š{location_text}" if unit else f"- {location_text}"
            if line not in seen_locations:
                seen_locations.add(line)
                location_lines.append(line)

    # Prepare Context (limit to first 500 chars per doc to avoid token overflow)
    context_text = ""
    for idx, doc in enumerate(context_docs['documents'][0]):
        meta = context_docs['metadatas'][0][idx]
        title = meta.get('title', 'ç„¡æ¨™é¡Œ')
        url = meta.get('url', '#')
        # Truncate long documents
        doc_preview = doc[:500] + "..." if len(doc) > 500 else doc
        context_text += f"\n--- è³‡æ–™ä¾†æº {idx+1}: [{title}]({url}) ---\n{doc_preview}\n"

    # Strict System Prompt
    location_hint = ""
    if location_lines:
        location_hint = "ã€è¾¦ç†åœ°é»ï¼ˆè‹¥é©ç”¨ï¼‰ã€‘\n" + "\n".join(location_lines) + "\n\n"

    # Identity Context Injection
    identity_instruction = ""
    if user_identity:
        identity_instruction = f"""
ã€ä½¿ç”¨è€…èº«åˆ†è³‡è¨Šã€‘
{user_identity}
è«‹å‹™å¿…æ ¹æ“šä¸Šè¿°ä½¿ç”¨è€…èº«åˆ†ï¼ˆå­¸é™¢/å­¸åˆ¶ï¼‰ï¼Œå„ªå…ˆæä¾›é©ç”¨çš„è¦å®šæˆ–æµç¨‹ã€‚è‹¥ä¸åŒèº«åˆ†æœ‰ä¸åŒè¦å®šï¼Œè«‹æ˜ç¢ºæŒ‡å‡ºã€‚
"""

    prompt = f"""ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„ã€Œå°å¤§è¡Œæ”¿å°åŠ©æ‰‹ã€ã€‚è«‹æ ¹æ“šä»¥ä¸‹æä¾›çš„ã€åƒè€ƒè³‡æ–™ã€‘ä¾†å›ç­”ä½¿ç”¨è€…çš„å•é¡Œã€‚

ã€å›ç­”å®ˆå‰‡ã€‘
1. ä½ çš„å›ç­”å¿…é ˆ**åš´æ ¼åŸºæ–¼**æä¾›çš„åƒè€ƒè³‡æ–™ã€‚å¦‚æœåƒè€ƒè³‡æ–™æ²’æœ‰æåŠï¼Œè«‹ç›´æ¥èªªã€ŒæŠ±æ­‰ï¼Œç›®å‰çš„è³‡æ–™åº«ä¸­æ²’æœ‰ç›¸é—œè³‡è¨Šã€ã€‚
2. è‹¥åƒè€ƒè³‡æ–™ä¸­æœ‰è¾¦ç†åœ°é»è³‡è¨Šï¼Œè«‹åœ¨å›ç­”é–‹é ­ä»¥ã€Œè¾¦ç†åœ°é»ï¼šã€åˆ—å‡ºï¼ˆå¯å¤šç­†ï¼‰ã€‚
3. å›ç­”è«‹æ¢ç†åˆ†æ˜ï¼Œä½¿ç”¨é»åˆ—å¼æ•´ç†é‡é»ã€‚
4. èªæ°£è«‹ä¿æŒè¦ªåˆ‡ã€å°ˆæ¥­ã€‚
5. è«‹ä½¿ç”¨ç¹é«”ä¸­æ–‡å›ç­”ã€‚
{identity_instruction}

ã€åƒè€ƒè³‡æ–™ã€‘
{location_hint}
{context_text}

ã€ä½¿ç”¨è€…å•é¡Œã€‘
{query}
"""
    
    def _safe_get_text(resp):
        """Safely extract text from Gemini response without throwing."""
        try:
            return resp.text
        except Exception:
            pass
        try:
            if resp.candidates:
                content = resp.candidates[0].content
                if content and content.parts:
                    return "".join([part.text for part in content.parts if getattr(part, "text", None)])
        except Exception:
            pass
        return ""

    import time
    from google.api_core import exceptions

    if not api_key:
        return "âš ï¸ è«‹å…ˆåœ¨å´é‚Šæ¬„è¼¸å…¥ Gemini API Key ä»¥å•Ÿç”¨ AI å›ç­”åŠŸèƒ½ã€‚", context_docs
        
    # Retry logic for Quota Exceeded (429)
    max_retries = 3
    retry_delay = 5  # Initial delay
    
    for attempt in range(max_retries):
        try:
            response = model.generate_content(prompt)
            answer = _safe_get_text(response)
            if answer:
                if location_lines:
                    location_block = "è¾¦ç†åœ°é»ï¼š\n" + "\n".join(location_lines) + "\n\n"
                    answer = location_block + answer
                return answer, context_docs
            # If answer is empty but no exception, return custom message or break to fail
            break 
        except exceptions.ResourceExhausted as e:
            if attempt < max_retries - 1:
                wait_time = retry_delay * (2 ** attempt)  # Exponential backoff
                st.warning(f"âš ï¸ è«‹æ±‚æ¬¡æ•¸éå¤š (Quota Exceeded)ï¼Œæ­£åœ¨ç­‰å¾… {wait_time} ç§’å¾Œé‡è©¦... (å˜—è©¦ {attempt+1}/{max_retries})")
                time.sleep(wait_time)
            else:
                return f"æŠ±æ­‰ï¼Œè«‹æ±‚æ¬¡æ•¸å·²é”ä¸Šé™ ({str(e)})ã€‚è«‹ç¨å¾Œå†è©¦æˆ–æª¢æŸ¥æ‚¨çš„ API Key é…é¡ã€‚", context_docs
        except Exception as e:
             # Other errors, fail immediately or handle appropriately
             error_msg = f"ç”Ÿæˆå›ç­”æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}"
             return f"æŠ±æ­‰ï¼Œç³»çµ±é‡åˆ°å•é¡Œï¼š{error_msg}\n\nè«‹ç¨å¾Œå†è©¦æˆ–è¯ç¹«ç®¡ç†å“¡ã€‚", context_docs
    
    return "æŠ±æ­‰ï¼ŒAI æ¨¡å‹æœªèƒ½ç”Ÿæˆå›ç­”ã€‚è«‹å˜—è©¦é‡æ–°æå•æˆ–ç°¡åŒ–å•é¡Œã€‚", context_docs

# --- UI LAYOUT ---
st.title("ğŸ“ è‡ºå¤§æ ¡åœ’è¡Œæ”¿å°å¹«æ‰‹ã€NTU Admin Helperã€‘")
st.markdown("æˆ‘æ˜¯ä½ çš„å°ˆå±¬åŠ©æ‰‹ï¼Œå¯ä»¥å›ç­”é—œæ–¼ **æ•™å‹™è™• (ACA)ã€å­¸å‹™è™• (OSA)ã€åœ–æ›¸é¤¨ (LIB)ã€ç¸½å‹™è™• (OGA)** çš„å„é¡è¡Œæ”¿å•é¡Œï¼")

# --- Session State Config ---
if "user_api_key" not in st.session_state:
    st.session_state.user_api_key = GEMINI_API_KEY

@st.dialog("ğŸ”‘ è¨­å®š Gemini API Key")
def api_key_dialog():
    st.write("è«‹è¼¸å…¥æ‚¨çš„ Google Gemini API Key ä»¥å•Ÿç”¨å®Œæ•´åŠŸèƒ½ã€‚")
    new_key = st.text_input(
        "API Key", 
        value=st.session_state.user_api_key, 
        type="password",
        help="å¾ Google AI Studio ç²å–"
    )
    if st.button("ç¢ºèªå„²å­˜"):
        st.session_state.user_api_key = new_key
        st.rerun()

@st.dialog("â„¹ï¸ ç³»çµ±è³‡è¨Šèˆ‡èªªæ˜")
def system_info_dialog():
    st.header("ğŸ’¡ é—œæ–¼æœ¬ç³»çµ±")
    st.success("âœ¨ BGE-M3 æœ¬åœ°æª¢ç´¢ï¼šä½¿ç”¨é«˜æ•ˆèƒ½å‘é‡æ¨¡å‹ï¼Œä¿éšœæª¢ç´¢æº–ç¢ºåº¦ã€‚")
    st.success("ğŸ”— Two-Stage Retrievalï¼šç¨å®¶é›™éšæ®µæª¢ç´¢æŠ€è¡“ï¼Œå…ˆé–å®šå–®ä½å†æ‰¾ç´°ç¯€ã€‚")
    st.success("ğŸ¤– Gemini æ™ºèƒ½å›ç­”ï¼šæ•´åˆ Google Gemini Proï¼Œæä¾›æµæš¢çš„èªè¨€ç”Ÿæˆã€‚")

    st.divider()

    st.header("ğŸ› ï¸ æŠ€è¡“æ¶æ§‹ï¼šTwo-Stage Retrieval")
    st.info("""
    **Two-Stage Retrieval (é›™éšæ®µæª¢ç´¢)** æ˜¯ä¸€ç¨®é‡å°è¡Œæ”¿å•ç­”å„ªåŒ–çš„ç­–ç•¥ï¼š

    1.  **ç¬¬ä¸€éšæ®µ (Stage 1)**ï¼š
        *   **ç›®æ¨™**ï¼šå¿«é€Ÿæƒæå…¨åŸŸè³‡æ–™åº«ã€‚
        *   **å‹•ä½œ**ï¼šæ‰¾å‡ºèˆ‡å•é¡Œæœ€ç›¸é—œçš„ 5 ç­†è³‡æ–™ï¼Œä¸¦åˆ†æå…¶ä¸­æåˆ°çš„ã€Œè¡Œæ”¿å–®ä½åç¨±ã€(å¦‚è¨»å†Šçµ„ã€èª²å¤–çµ„)ã€‚
        
    2.  **ç¬¬äºŒéšæ®µ (Stage 2)**ï¼š
        *   **ç›®æ¨™**ï¼šæ·±åº¦æŒ–æ˜ç‰¹å®šå–®ä½çš„è³‡è¨Šã€‚
        *   **å‹•ä½œ**ï¼šå¼·åˆ¶é–å®šç¬¬ä¸€éšæ®µæ‰¾åˆ°çš„å–®ä½ï¼Œé¡å¤–æª¢ç´¢è©²å–®ä½çš„ã€Œåœ°é»ã€ã€ã€Œé›»è©±ã€ã€ã€Œè©³ç´°è¦ç« ã€ã€‚
        
    ğŸ¯ **å„ªé»**ï¼šè§£æ±ºäº†ã€Œè·¨æ–‡æª”åƒç…§ã€çš„å•é¡Œã€‚ä¾‹å¦‚ç•¶ä½ å•ã€Œè¨»å†Šçµ„åœ¨å“ªï¼Ÿã€ï¼Œç³»çµ±èƒ½ç²¾æº–é—œè¯åˆ°è¨»å†Šçµ„çš„ä½ç½®è³‡è¨Šï¼Œè€Œä¸æœƒè¢«å…¶ä»–å–®ä½çš„æ–‡ç« å¹²æ“¾ã€‚
    """)

    st.divider()

    st.header("ğŸ“ åœ°åœ–æœå‹™")
    st.markdown("""
    æœ¬ç³»çµ±æ•´åˆäº† **å°å¤§æ ¡åœ’åœ°åœ– API**ã€‚
    ç•¶æ‚¨çš„å•é¡ŒåŒ…å«ã€Œåœ¨å“ªè£¡ã€ã€ã€Œä½ç½®ã€ç­‰é—œéµå­—æ™‚ï¼Œç³»çµ±æœƒè‡ªå‹•ï¼š
    1. åˆ†æå›ç­”ä¸­æåˆ°çš„å»ºç¯‰ç‰© (å¦‚ï¼šè¡Œæ”¿å¤§æ¨“)ã€‚
    2. æŠ“å–è©²å»ºç¯‰ç‰©çš„ç¶“ç·¯åº¦ã€‚
    3. åœ¨å›ç­”ä¸‹æ–¹ç›´æ¥ç¹ªè£½äº’å‹•å¼åœ°åœ–ã€‚
    """)



# --- Chat Logic Function ---
def handle_query(query_text):
    """
    Handle user query: display message, retrieve context, generate answer, and update history.
    """
    # Use session state for context variables (safe access before sidebar render)
    college_opt = st.session_state.get("college_opt", "å…¶ä»–å­¸é™¢ (ä¸€èˆ¬)")
    degree_opt = st.session_state.get("degree_opt", "å­¸å£«ç­")
    model_name = st.session_state.get("model_name", GEMINI_MODEL)
    api_key_val = st.session_state.get("user_api_key", "")

    # 1. Display User Message
    with st.chat_message("user"):
        st.markdown(query_text)
    st.session_state.messages.append({"role": "user", "content": query_text})

    # 2. Assistant Logic
    with st.chat_message("assistant"):
        with st.spinner("æ­£åœ¨æ™ºæ…§æª¢ç´¢ä¸­...(ä½¿ç”¨ Two-Stage Retrieval)"):
            engine = get_rag_engine()
            if engine:
                # Construct Context-Aware Query
                context_suffix = ""
                if college_opt == "é†«å­¸é™¢/å…¬å…±è¡›ç”Ÿå­¸é™¢":
                    context_suffix += " (é†«å­¸é™¢/å…¬è¡›å­¸é™¢è¦å®š)"
                
                context_suffix += f" ({degree_opt})"
                
                # Construct identity string for LLM
                user_identity_str = f"- å­¸é™¢ï¼š{college_opt}\n- å­¸åˆ¶ï¼š{degree_opt}"
                
                # 1. Rewrite with context (last 3 turns)
                rewritten_prompt = rewrite_query_with_context(
                    api_key_val,
                    model_name,
                    st.session_state.messages,
                    query_text
                )
                
                # Combine rewritten conversational query with filter context
                final_search_query = f"{rewritten_prompt} {context_suffix}"
                
                print(f"DEBUG: Final Search Query: {final_search_query}")
                print(f"DEBUG: User Identity: {user_identity_str}")
                
                # 2. Retrieve using Enhanced RAG Engine
                print("DEBUG: Starting retrieval...")
                results = retrieve_documents(engine, final_search_query, use_two_stage=True)
                print("DEBUG: Retrieval complete. Results found:", len(results.get('documents', [[]])[0]))
                
                # 3. Generate
                if api_key_val:
                    print(f"DEBUG: Generating response with model {model_name}...")
                    # Pass identity context
                    answer, sources = generate_response(
                        api_key_val, 
                        model_name, 
                        query_text, 
                        results,
                        user_identity=user_identity_str
                    )
                    print("DEBUG: Generation complete.")
                else:
                    answer = "âš ï¸ è«‹å…ˆåœ¨å´é‚Šæ¬„è¼¸å…¥ Gemini API Key ä»¥å•Ÿç”¨ AI å›ç­”åŠŸèƒ½"
                    sources = results
                
                # 4. Show Answer
                st.markdown(answer)
                
                # 5. Extract Map Data
                print("DEBUG MAP: Starting automatic map generation")
                map_service = get_map_service_cached()
                
                documents = []
                for idx in range(len(sources['documents'][0])):
                    documents.append({
                        'content': sources['documents'][0][idx],
                        'metadata': sources['metadatas'][0][idx]
                    })
                
                buildings_found = map_service.extract_buildings_from_metadata(documents)
                print(f"DEBUG MAP: Buildings extracted: {buildings_found}")
                
                # Display map IMMEDIATELY for this turn
                if buildings_found:
                    st.divider()
                    st.subheader("ğŸ“ ç›¸é—œä½ç½®åœ°åœ–")
                    campus_map = map_service.create_map(buildings_found)
                    if campus_map:
                        # Use a special key to avoid conflicts
                        st_folium(campus_map, width=700, height=500, key=f"current_map_{int(time.time())}")
                        st.caption(f"é¡¯ç¤º {len(buildings_found)} å€‹å»ºç¯‰ç‰©: {', '.join(buildings_found)}")
                    else:
                        st.info("ğŸ’¡ å»ºç¯‰ç‰©åº§æ¨™è³‡è¨Šä¸å®Œæ•´ï¼Œç„¡æ³•é¡¯ç¤ºåœ°åœ–")
                
                # 6. Show Sources
                with st.expander("æŸ¥çœ‹åƒè€ƒä¾†æº"):
                     for idx, meta in enumerate(sources['metadatas'][0]):
                        st.markdown(f"**{idx+1}. [{meta.get('title')}]({meta.get('url')})**")
                        st.caption(f"ä¾†è‡ª: {meta.get('department').upper()}")

                # Save to history INCLUDING buildings
                st.session_state.messages.append({
                    "role": "assistant", 
                    "content": answer,
                    "sources": sources,
                    "buildings": buildings_found
                })
                
                # Force Garbage Collection
                gc.collect()

# Sidebar
with st.sidebar:
    # 1. API Key (Popout Dialog)
    st.header("ğŸ”‘ API è¨­å®š")
    
    if st.button("è¨­å®š Gemini API Key", use_container_width=True, icon="âš™ï¸"):
        api_key_dialog()
            
    if st.session_state.user_api_key:
        st.markdown(
            """
            <div style='background-color: #d1e7dd; color: #0f5132; padding: 0.75rem 1rem; border-radius: 0.375rem; text-align: center; margin-bottom: 1rem; font-weight: bold;'>
                âœ… API Key å·²å•Ÿç”¨ âœ…
            </div>
            """, 
            unsafe_allow_html=True
        )
    else:
        st.markdown(
            """
            <div style='background-color: #f8d7da; color: #842029; padding: 0.75rem 1rem; border-radius: 0.375rem; text-align: center; margin-bottom: 1rem; font-weight: bold;'>
                âŒ API Key æœªè¨­å®š âŒ
            </div>
            """,
            unsafe_allow_html=True
        )

    # Update global variable for downstream use
    user_api_key = st.session_state.user_api_key

    st.divider()

    # 2. Common Questions (Moved to Top)
    st.header("ğŸ’¡ å¸¸è¦‹å•é¡Œå¿«é¸")
    col1, col2 = st.columns(2)
    with col1:
        if st.button("ğŸ˜­å¥½æƒ³åœä¿®"):
            st.session_state.pending_query = "å¦‚ä½•è¾¦ç†åœä¿®èª²ç¨‹ï¼Ÿ"
        if st.button("ğŸ’°å­¸ç”Ÿä¿éšªæ€éº¼è«‹"):
            st.session_state.pending_query = "å¦‚ä½•ç”³è«‹å­¸ç”Ÿåœ˜é«”ä¿éšªç†è³ ï¼Ÿ"
    with col2:
        if st.button("ğŸ“„æˆ‘è¦å°æˆç¸¾å–®"):
            st.session_state.pending_query = "å¦‚ä½•ç”³è«‹ä¸­æ–‡æˆç¸¾å–®ï¼Ÿ"
        if st.button("ğŸ“–åœ–æ›¸é¤¨åˆ°å¹¾é»"):
            st.session_state.pending_query = "ç¸½åœ–æ›¸é¤¨é–‹æ”¾æ™‚é–“ç‚ºä½•ï¼Ÿ"

    st.divider()

    # 3. Identity Settings
    st.header("ğŸ‘¤ èº«åˆ†è¨­å®š")
    college_option = st.selectbox(
        "å­¸é™¢åˆ¥",
        ["å…¶ä»–å­¸é™¢ (ä¸€èˆ¬)", "é†«å­¸é™¢/å…¬å…±è¡›ç”Ÿå­¸é™¢"],
        index=0,
        help="é†«å­¸é™¢èˆ‡å…¬è¡›å­¸é™¢ä¹‹æ•™å‹™è¦å®šå¯èƒ½æœ‰æ‰€ä¸åŒ",
        key="college_opt"
    )
    degree_option = st.selectbox(
        "å­¸åˆ¶åˆ¥",
        ["å­¸å£«ç­", "ç¢©å£«ç­", "åšå£«ç­"],
        index=0,
        key="degree_opt"
    )

    st.divider()

    # 4. Model Selection
    st.header("ğŸ¤– æ¨¡å‹è¨­å®š")
    user_model_name = st.text_input(
        "Gemini Model Name",
        value=GEMINI_MODEL,
        help="ä¾‹å¦‚: gemini-1.5-flash, gemini-2.0-flash",
        key="model_name"
    )

    if st.button("åˆ—å‡ºå¯ç”¨æ¨¡å‹"):
        if not user_api_key:
            st.error("è«‹å…ˆè¼¸å…¥ API Key")
        else:
            try:
                genai.configure(api_key=user_api_key)
                models = [m.name for m in genai.list_models() if 'generateContent' in m.supported_generation_methods]
                st.success(f"æ‰¾åˆ° {len(models)} å€‹å¯ç”¨æ¨¡å‹:")
                st.write(models)
            except Exception as e:
                st.error(f"æŸ¥è©¢å¤±æ•—: {e}")

    st.divider()
    if st.button("â„¹ï¸ ç³»çµ±è³‡è¨Š", use_container_width=True):
        system_info_dialog()

    st.divider()
    
    # 5. Optimization Settings
    st.header("ğŸš€ æ•ˆèƒ½å„ªåŒ–")
    if st.button("ğŸ—‘ï¸ æ¸…é™¤å°è©±ç´€éŒ„", use_container_width=True):
        st.session_state.messages = []
        st.experimental_rerun()
        
    show_history_maps = st.toggle(
        "é¡¯ç¤ºæ­·å²åœ°åœ–", 
        value=False, 
        help="é–‹å•Ÿå¾Œæœƒé¡¯ç¤ºæ­·å²è¨Šæ¯ä¸­çš„äº’å‹•åœ°åœ–ï¼ˆè¼ƒåƒè³‡æºï¼‰ã€‚é—œé–‰å¯é¿å…æ‡‰ç”¨ç¨‹å¼å¡é “ã€‚"
    )

# Handle Sidebar Button Clicks (Main Area Output)
if "pending_query" in st.session_state and st.session_state.pending_query:
    handle_query(st.session_state.pending_query)
    st.session_state.pending_query = None  # Reset

# Chat Interface
if "messages" not in st.session_state:
    st.session_state.messages = []

# Display Chat History
for idx, message in enumerate(st.session_state.messages):
    with st.chat_message(message["role"]):
        st.markdown(message["content"])
        
        # Restore Map from History
        if "buildings" in message and message["buildings"]:
            st.caption(f"ğŸ“ ç›¸é—œä½ç½®: {', '.join(message['buildings'])}")
            
            # Only render map if toggle is ON
            if show_history_maps:
                try:
                    map_service = get_map_service_cached()
                    historical_map = map_service.create_map(message["buildings"], center_on_first=True)
                    if historical_map:
                        st_folium(historical_map, width=700, height=400, key=f"history_map_{idx}")
                except Exception as e:
                    st.error(f"ç„¡æ³•è¼‰å…¥åœ°åœ–: {e}")
            else:
                 st.caption("(å·²éš±è—åœ°åœ–ä»¥ç¯€çœè³‡æºï¼Œè«‹è‡³å´é‚Šæ¬„é–‹å•Ÿã€Œé¡¯ç¤ºæ­·å²åœ°åœ–ã€)")

        # Show specific sources if available
        if "sources" in message:
            with st.expander("æŸ¥çœ‹åƒè€ƒä¾†æº"):
                for s_idx, meta in enumerate(message["sources"]['metadatas'][0]):
                    st.markdown(f"**{s_idx+1}. [{meta.get('title')}]({meta.get('url')})**")
                    st.caption(f"ä¾†è‡ª: {meta.get('department').upper()}")

# User Input
if prompt := st.chat_input("è«‹è¼¸å…¥ä½ çš„å•é¡Œ (ä¾‹å¦‚ï¼šä¼‘å­¸è¦æ€éº¼ç”³è«‹ï¼Ÿ)"):
    handle_query(prompt)


